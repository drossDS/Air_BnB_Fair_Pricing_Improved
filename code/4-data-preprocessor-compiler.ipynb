{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194b39bd-9f14-46fd-b254-6e32ec84ffb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# NOTEBOOK 4 - DATA PREPROCESSOR AND COMPILER:\n",
    "## Perform All Data Transformations From Notebooks 1, 2, nd 3 in One Location\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502e66-9cbd-42ff-92ee-6054ad655c44",
   "metadata": {},
   "source": [
    "# PROJECT : Boston Airbnb Fair Pricing Tool and Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26424b92-d911-4a5b-bfed-83c32ab567fd",
   "metadata": {},
   "source": [
    "# Notebook Overview:\n",
    "* All data transformation steps from Notebooks 1, 2, and 3 are recreated as individual functions below and then all run under one master function\n",
    "* Provisions are added so that the training data will store certain values and train certain algorithms such that the exact same transformations are performed on validation and test data\n",
    "* Finally, all datasets are exported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e018b92-4aa4-4987-87c1-88bf991a5a85",
   "metadata": {},
   "source": [
    "# NOTE REGARDING GOOGLE GEOCODING API DATA\n",
    "The data collected via the Geocoding API are not pushed to GitHub as Google appears to have restrictions on the data obtained.  The [Google Maps Platform Terms of Service](https://cloud.google.com/maps-platform/terms?_gl=1*1v1wn86*_ga*NjA0MzgzMTI1LjE2ODU4NjA1NTU.*_ga_NRWSTWS78N*MTY4NjMyODIwNy44LjEuMTY4NjMyODQ3MS4wLjAuMA..), section 3.2.3(a) does not appear to permit the user to '(i) pre-fetch, index, store, reshare, or rehost Google Maps Content outside the services.'  This may imply that storing the geocoding data outside of this local machine where others can access it, is not allowed.\n",
    "\n",
    "Thus, while it's not fully clear why Google would offer these services but disallow the publication of its already-publicly available data, the geocode data will be omitted from the public repo.\n",
    "\n",
    "#### In this notebook, these data are imported from the local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f165c-7c59-4f83-8351-bba6bc00743a",
   "metadata": {},
   "source": [
    "# 1 - Library Imports and Read-in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8823ede5-8674-46fa-9790-46af979d8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For error hansling see source provided by H. Butler:\n",
    "#  Source:  https://stackoverflow.com/questions/1483429/how-do-i-print-an-exception-in-python\n",
    "import traceback\n",
    "\n",
    "# Import Math for help with distances\n",
    "import math\n",
    "\n",
    "# For tesxt:\n",
    "import re\n",
    "\n",
    "# For times:\n",
    "import time\n",
    "\n",
    "# Set a random seed for imputation\n",
    "#  Source:  https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d00783-8649-4dd6-91d7-876943982bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Training Data\n",
    "lstn = pd.read_csv('../data/listings_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e79290-03e3-462e-b8ad-130dc7c8c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this file has been added to the gitignore file and is NOT located in the repository\n",
    "geodata = pd.read_csv('../data/lat_lng_data.csv')\n",
    "\n",
    "latitudes = list(geodata.lat)\n",
    "longitudes = list(geodata.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2194d96-0848-4b94-9ad0-c4ba11c058a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Drop Un-needed Columns\n",
    "The below function drops all columns previously identified to be dropped and not used.  Some columns are dropped in a second step which may later be kept in a subsequent iteration of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2494824a-083e-4b90-a6b3-5bc320603d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dropper(data_frame):\n",
    "    data_frame.drop(columns = [\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'source',\n",
    "        'picture_url', 'host_url', 'host_name', 'host_thumbnail_url', 'host_picture_url',\n",
    "        'neighbourhood','neighbourhood_group_cleansed', 'minimum_minimum_nights',\n",
    "        'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', \n",
    "        'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated',\n",
    "        'calendar_last_scraped', 'bathrooms', 'first_review', 'last_review',\n",
    "        'id', 'host_id',\n",
    "    ], inplace = True)\n",
    "\n",
    "    # For now, these columsn will also be dropped unless time allows for them to be processed:\n",
    "    data_frame.drop(columns = [\n",
    "        'host_location', 'host_neighbourhood', 'review_scores_rating', 'review_scores_accuracy',\n",
    "        'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "        'review_scores_location', 'review_scores_value', 'license'\n",
    "    ], inplace = True)\n",
    "    \n",
    "    # No need to reutrn the dataframe as the inplace functions carry over to the input dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f6646-3453-4b85-bc57-02ac131f1ffb",
   "metadata": {},
   "source": [
    "# 3 - Fix Datatypes\n",
    "Explanation provided in-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e20e3-fd0f-4884-933c-2fcde3858fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since the null values are not a very big percentage of the total data (though not a small percentage either),\n",
    "the data will be imputed with the median value.\n",
    "\n",
    "In order to do that, the percetnages need to be convereted where possible, so that the nulls can be imputed\n",
    "with the median value.\n",
    "\n",
    "The pcnt_floater functionwill be copied over here.  This is necessary as there are null values in these columns\n",
    "which cannot be simply converted within the lambda function because there is no percentage sign.\n",
    "'''\n",
    "\n",
    "# This function will attempt to convert a string percentage value into a float\n",
    "#  Source for help:  https://www.w3schools.com/python/python_try_except.asp\n",
    "def pcnt_floater(x):\n",
    "    try:\n",
    "        return float(x.replace('%', '').strip())\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea8d2bc-8076-442b-972a-a6e0ede289b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fixer(data_frame):\n",
    "\n",
    "# FIX PRICE:  The dolar signs must be removed from the prices and numbers converted to float values\n",
    "    data_frame.price = data_frame.price.apply(lambda x: float(x.replace('$','').replace(',','').strip()))\n",
    "\n",
    "    # FIX HOST SINCE:  Convert to datetime then to epoch time in days\n",
    "    '''\n",
    "    The method used below to convert to epoch time was discovered with the help of ChatGPT.\n",
    "    Per the lead instructor, it is ok to use ChatGPT is a search tool provided that we provide the\n",
    "    question that was asked:\n",
    "\n",
    "    Question:  'in python, I want to convert a pandas datetime object to epoch time'\n",
    "\n",
    "    Additional help from:  https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n",
    "    '''\n",
    "\n",
    "    # The strings are converted to date time, then to epoch time with '.timestamp()'\n",
    "    # The epoch time is then divided by the product the number of hours and seconds per day\n",
    "    #   to get the number of days since the epoch time origin \n",
    "    data_frame['host_since'] = pd.to_datetime(data_frame['host_since']).apply(lambda x: x.timestamp()/(3600*24))\n",
    "\n",
    "    # FIX RESPONSE & ACCEPTANCE RATES:  Remove percentages\n",
    "    # Convert percentages where they can be converted\n",
    "    data_frame.host_acceptance_rate = data_frame.host_acceptance_rate.apply(lambda x: pcnt_floater(x))\n",
    "    data_frame.host_response_rate = data_frame.host_response_rate.apply(lambda x: pcnt_floater(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fae4b4-de3c-4a51-9219-b9864f0f1147",
   "metadata": {},
   "source": [
    "# 4 - Impute Missing Data\n",
    "The function below will impute values and where necessary, store the imputation values from the training dataset for later imputation into validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db115cfb-62c6-43e0-a7c8-4b3cd3bea169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_imputer(data_frame, training_data):\n",
    "    # Impute missing text information with 'no_text_entered' into the following columns\n",
    "    nte_cols = ['description', 'neighborhood_overview','host_about', 'host_response_time']\n",
    "\n",
    "    for col in nte_cols:\n",
    "        data_frame[col].fillna('no_text_entered', inplace = True)\n",
    "\n",
    "    # Impute missing data with the median in the following columns\n",
    "    median_cols = ['host_response_rate', 'host_acceptance_rate', 'bedrooms', 'beds']\n",
    "    \n",
    "    for col in median_cols:\n",
    "        \n",
    "        # Store the median values to a global varialbe for each column only if the it's a training dataset\n",
    "        if training_data == True:\n",
    "            globals()[f'median_val_{col}'] = data_frame[col].median()\n",
    "        \n",
    "        data_frame[col].fillna(globals()[f'median_val_{col}'], inplace = True)\n",
    "\n",
    "    # Impute missing values with the mode in the following columns\n",
    "    data_frame.host_is_superhost.fillna(data_frame.host_is_superhost.mode()[0], inplace = True)\n",
    "    data_frame.bathrooms_text.fillna(data_frame.bathrooms_text.mode()[0], inplace = True)\n",
    "\n",
    "    # Impute missing data with 0 in reviews per month\n",
    "    data_frame.reviews_per_month.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa3f762-c42e-4967-8c5a-0fdecb1560d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify imputation\n",
    "sum(lstn.isnull().sum() != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99dab65-a769-4a5f-b7cb-e9e576d2a5a5",
   "metadata": {},
   "source": [
    "# 5 - Create Simple Numerical Features\n",
    "The function below adds the calculated percentages of each host's inventory of different room types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d007ca-317c-456f-acb9-834858054ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simp_num_ft(data_frame):\n",
    "    # Create percentage columns for the calculated listings by listing type\n",
    "    data_frame['pcnt_ent_homes'] = round(data_frame['calculated_host_listings_count_entire_homes'] / data_frame['calculated_host_listings_count'], 3)\n",
    "    data_frame['pcnt_private'] = round(data_frame['calculated_host_listings_count_private_rooms'] / data_frame['calculated_host_listings_count'], 3)\n",
    "    data_frame['pcnt_shared'] = round(data_frame['calculated_host_listings_count_shared_rooms'] / data_frame['calculated_host_listings_count'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8a3d1-4fac-4cc9-8607-5fc57c86db61",
   "metadata": {},
   "source": [
    "# 6 - Add T-Stop Distnace Data\n",
    "Performs the same functions from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c0049c-fa6c-4965-b15b-ff94e5493bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funciton was written around the following source:\n",
    "# https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "\n",
    "def min_dist(fn_lat, fn_lng, lat_data, lng_data):\n",
    "    \n",
    "    '''\n",
    "    The average latitude of all Boston Airbnb listings is about 42.3 degrees.  At this latitude,\n",
    "    one degree of latitude is a larger distance than one degree of longitude.  Thus, if the\n",
    "    latitude and longitude angular displacements between two locations are equal, their distances\n",
    "    are not.  A distance correction factor needs to be added in to correct this discrepancy.  It\n",
    "    will be multiplied by the longitude values to get latitidue and longitude in the same equivalent\n",
    "    distance scale.\n",
    "    '''\n",
    "    \n",
    "    # Source for calculating cosine\n",
    "    # https://www.geeksforgeeks.org/python-math-cos-function/\n",
    "    # Source for pi:  https://www.w3schools.com/python/ref_math_pi.asp\n",
    "    dist_correction_factor = math.cos(42.3*math.pi/180)\n",
    "    \n",
    "    # Set a minimum distnace well beyond anything that would be derived\n",
    "    min_dist = 90\n",
    "    \n",
    "    # Write a loop to find the minimum (euclidean) distance to every T-stop\n",
    "    for n in range(len(latitudes)):\n",
    "        dist = ((fn_lat - lat_data[n])**2 + (dist_correction_factor * (fn_lng - lng_data[n]))**2)**0.5\n",
    "        \n",
    "        # Store this distance if smaller than min distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    \n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0e77c8-999e-4fd7-9043-90d207678ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This funciton was written around the following source:\n",
    "# # https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "\n",
    "# def min_dist(fn_lat, fn_lng, lat_data, lng_data):\n",
    "    \n",
    "#     # Set a minimum distnace well beyond anything that would be derived\n",
    "#     min_dist = 90\n",
    "    \n",
    "#     # Write a loop to find the minimum (euclidean) distance to every T-stop\n",
    "#     for n in range(len(latitudes)):\n",
    "#         dist = ((fn_lat - lat_data[n])**2 + (fn_lng - lng_data[n])**2)**0.5\n",
    "        \n",
    "#         # Store this distance if smaller than min distance\n",
    "#         if dist < min_dist:\n",
    "#             min_dist = dist\n",
    "    \n",
    "#     return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce236d8d-e4bd-41bb-8d5e-756285ae7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_ft_adder(data_frame):\n",
    "    # Crate a new column with the minimum distance to any T-stop\n",
    "    #  The following source was used to help write this code (note axis = 1 is KEY!):\n",
    "        # https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "    data_frame['min_distance'] = data_frame.apply(lambda x: min_dist(x.latitude, x.longitude, latitudes, longitudes), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f02535-a716-4953-b428-309d23421646",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7 - Create Log Features\n",
    "For all numerical columns, a feature that is equal to the logarithm of a numerical feature will be added for each numerical feature.  As the histograms and correlation plots in Notebook 1 showed, some features achieved higher correlations coefficients with log_price after themselves being log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35d4d080-0a58-4205-98ab-12c1b638048e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>2.462554e+17</td>\n",
       "      <td>3.427382e+17</td>\n",
       "      <td>3.781000e+03</td>\n",
       "      <td>2.515811e+07</td>\n",
       "      <td>4.824698e+07</td>\n",
       "      <td>6.470705e+17</td>\n",
       "      <td>8.493358e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrape_id</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>2.023032e+13</td>\n",
       "      <td>8.399948e-01</td>\n",
       "      <td>2.023032e+13</td>\n",
       "      <td>2.023032e+13</td>\n",
       "      <td>2.023032e+13</td>\n",
       "      <td>2.023032e+13</td>\n",
       "      <td>2.023032e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_id</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>1.499206e+08</td>\n",
       "      <td>1.415986e+08</td>\n",
       "      <td>4.804000e+03</td>\n",
       "      <td>2.234822e+07</td>\n",
       "      <td>1.074344e+08</td>\n",
       "      <td>2.758496e+08</td>\n",
       "      <td>5.041795e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_listings_count</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>4.690273e+02</td>\n",
       "      <td>1.336724e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>8.400000e+01</td>\n",
       "      <td>4.807000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_total_listings_count</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>6.275074e+02</td>\n",
       "      <td>1.505024e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.270000e+02</td>\n",
       "      <td>5.358000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_group_cleansed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>4.233701e+01</td>\n",
       "      <td>2.736836e-02</td>\n",
       "      <td>4.223530e+01</td>\n",
       "      <td>4.232098e+01</td>\n",
       "      <td>4.234448e+01</td>\n",
       "      <td>4.235431e+01</td>\n",
       "      <td>4.239132e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>-7.108231e+01</td>\n",
       "      <td>3.305332e-02</td>\n",
       "      <td>-7.117349e+01</td>\n",
       "      <td>-7.110036e+01</td>\n",
       "      <td>-7.107307e+01</td>\n",
       "      <td>-7.106081e+01</td>\n",
       "      <td>-7.099600e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>3.167566e+00</td>\n",
       "      <td>2.213750e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>2438.0</td>\n",
       "      <td>1.754307e+00</td>\n",
       "      <td>1.219764e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beds</th>\n",
       "      <td>2711.0</td>\n",
       "      <td>1.790483e+00</td>\n",
       "      <td>1.440709e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>3.099892e+01</td>\n",
       "      <td>3.943985e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>4.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>6.712352e+02</td>\n",
       "      <td>4.524375e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>3.680000e+02</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>1.125000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_minimum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>3.472816e+01</td>\n",
       "      <td>5.466348e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>4.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_minimum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>6.089356e+01</td>\n",
       "      <td>1.010902e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>9.100000e+01</td>\n",
       "      <td>4.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_maximum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>2.317402e+06</td>\n",
       "      <td>7.050721e+07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>2.147484e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_maximum_nights</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>7.722815e+06</td>\n",
       "      <td>1.285657e+08</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>2.147484e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimum_nights_avg_ntm</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>5.911309e+01</td>\n",
       "      <td>9.852488e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>9.100000e+01</td>\n",
       "      <td>4.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximum_nights_avg_ntm</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>4.296836e+06</td>\n",
       "      <td>8.475779e+07</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.650000e+02</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>2.147484e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calendar_updated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_30</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>1.009133e+01</td>\n",
       "      <td>1.079859e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_60</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>2.447213e+01</td>\n",
       "      <td>2.177730e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>6.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_90</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>4.046674e+01</td>\n",
       "      <td>3.204159e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>6.900000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_365</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>1.990065e+02</td>\n",
       "      <td>1.303065e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>2.210000e+02</td>\n",
       "      <td>3.220000e+02</td>\n",
       "      <td>3.650000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>4.108738e+01</td>\n",
       "      <td>8.199650e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>8.210000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews_ltm</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>1.162100e+01</td>\n",
       "      <td>2.122723e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.550000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews_l30d</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>6.238763e-01</td>\n",
       "      <td>1.504259e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_rating</th>\n",
       "      <td>1972.0</td>\n",
       "      <td>4.688463e+00</td>\n",
       "      <td>4.660137e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.600000e+00</td>\n",
       "      <td>4.800000e+00</td>\n",
       "      <td>4.970000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <td>1966.0</td>\n",
       "      <td>4.752157e+00</td>\n",
       "      <td>3.985007e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.700000e+00</td>\n",
       "      <td>4.860000e+00</td>\n",
       "      <td>4.990000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <td>1967.0</td>\n",
       "      <td>4.728124e+00</td>\n",
       "      <td>3.940967e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.640000e+00</td>\n",
       "      <td>4.850000e+00</td>\n",
       "      <td>4.990000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <td>1966.0</td>\n",
       "      <td>4.824710e+00</td>\n",
       "      <td>3.709812e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.810000e+00</td>\n",
       "      <td>4.940000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_communication</th>\n",
       "      <td>1967.0</td>\n",
       "      <td>4.805358e+00</td>\n",
       "      <td>3.849088e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.790000e+00</td>\n",
       "      <td>4.920000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_location</th>\n",
       "      <td>1966.0</td>\n",
       "      <td>4.758032e+00</td>\n",
       "      <td>3.518585e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.670000e+00</td>\n",
       "      <td>4.875000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_scores_value</th>\n",
       "      <td>1966.0</td>\n",
       "      <td>4.596445e+00</td>\n",
       "      <td>4.603274e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.500000e+00</td>\n",
       "      <td>4.700000e+00</td>\n",
       "      <td>4.860000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>6.270802e+01</td>\n",
       "      <td>1.108793e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>3.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculated_host_listings_count_entire_homes</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>3.951097e+01</td>\n",
       "      <td>9.364456e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>3.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculated_host_listings_count_private_rooms</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>2.313305e+01</td>\n",
       "      <td>7.136186e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.740000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculated_host_listings_count_shared_rooms</th>\n",
       "      <td>2781.0</td>\n",
       "      <td>1.330457e-02</td>\n",
       "      <td>1.320940e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_per_month</th>\n",
       "      <td>1972.0</td>\n",
       "      <td>1.681521e+00</td>\n",
       "      <td>1.910530e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>2.700000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.420000e+00</td>\n",
       "      <td>1.629000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               count          mean  \\\n",
       "id                                            2781.0  2.462554e+17   \n",
       "scrape_id                                     2781.0  2.023032e+13   \n",
       "host_id                                       2781.0  1.499206e+08   \n",
       "host_listings_count                           2781.0  4.690273e+02   \n",
       "host_total_listings_count                     2781.0  6.275074e+02   \n",
       "neighbourhood_group_cleansed                     0.0           NaN   \n",
       "latitude                                      2781.0  4.233701e+01   \n",
       "longitude                                     2781.0 -7.108231e+01   \n",
       "accommodates                                  2781.0  3.167566e+00   \n",
       "bathrooms                                        0.0           NaN   \n",
       "bedrooms                                      2438.0  1.754307e+00   \n",
       "beds                                          2711.0  1.790483e+00   \n",
       "minimum_nights                                2781.0  3.099892e+01   \n",
       "maximum_nights                                2781.0  6.712352e+02   \n",
       "minimum_minimum_nights                        2781.0  3.472816e+01   \n",
       "maximum_minimum_nights                        2781.0  6.089356e+01   \n",
       "minimum_maximum_nights                        2781.0  2.317402e+06   \n",
       "maximum_maximum_nights                        2781.0  7.722815e+06   \n",
       "minimum_nights_avg_ntm                        2781.0  5.911309e+01   \n",
       "maximum_nights_avg_ntm                        2781.0  4.296836e+06   \n",
       "calendar_updated                                 0.0           NaN   \n",
       "availability_30                               2781.0  1.009133e+01   \n",
       "availability_60                               2781.0  2.447213e+01   \n",
       "availability_90                               2781.0  4.046674e+01   \n",
       "availability_365                              2781.0  1.990065e+02   \n",
       "number_of_reviews                             2781.0  4.108738e+01   \n",
       "number_of_reviews_ltm                         2781.0  1.162100e+01   \n",
       "number_of_reviews_l30d                        2781.0  6.238763e-01   \n",
       "review_scores_rating                          1972.0  4.688463e+00   \n",
       "review_scores_accuracy                        1966.0  4.752157e+00   \n",
       "review_scores_cleanliness                     1967.0  4.728124e+00   \n",
       "review_scores_checkin                         1966.0  4.824710e+00   \n",
       "review_scores_communication                   1967.0  4.805358e+00   \n",
       "review_scores_location                        1966.0  4.758032e+00   \n",
       "review_scores_value                           1966.0  4.596445e+00   \n",
       "calculated_host_listings_count                2781.0  6.270802e+01   \n",
       "calculated_host_listings_count_entire_homes   2781.0  3.951097e+01   \n",
       "calculated_host_listings_count_private_rooms  2781.0  2.313305e+01   \n",
       "calculated_host_listings_count_shared_rooms   2781.0  1.330457e-02   \n",
       "reviews_per_month                             1972.0  1.681521e+00   \n",
       "\n",
       "                                                       std           min  \\\n",
       "id                                            3.427382e+17  3.781000e+03   \n",
       "scrape_id                                     8.399948e-01  2.023032e+13   \n",
       "host_id                                       1.415986e+08  4.804000e+03   \n",
       "host_listings_count                           1.336724e+03  1.000000e+00   \n",
       "host_total_listings_count                     1.505024e+03  1.000000e+00   \n",
       "neighbourhood_group_cleansed                           NaN           NaN   \n",
       "latitude                                      2.736836e-02  4.223530e+01   \n",
       "longitude                                     3.305332e-02 -7.117349e+01   \n",
       "accommodates                                  2.213750e+00  1.000000e+00   \n",
       "bathrooms                                              NaN           NaN   \n",
       "bedrooms                                      1.219764e+00  1.000000e+00   \n",
       "beds                                          1.440709e+00  1.000000e+00   \n",
       "minimum_nights                                3.943985e+01  1.000000e+00   \n",
       "maximum_nights                                4.524375e+02  2.000000e+00   \n",
       "minimum_minimum_nights                        5.466348e+01  1.000000e+00   \n",
       "maximum_minimum_nights                        1.010902e+02  1.000000e+00   \n",
       "minimum_maximum_nights                        7.050721e+07  1.000000e+00   \n",
       "maximum_maximum_nights                        1.285657e+08  2.000000e+00   \n",
       "minimum_nights_avg_ntm                        9.852488e+01  1.000000e+00   \n",
       "maximum_nights_avg_ntm                        8.475779e+07  2.000000e+00   \n",
       "calendar_updated                                       NaN           NaN   \n",
       "availability_30                               1.079859e+01  0.000000e+00   \n",
       "availability_60                               2.177730e+01  0.000000e+00   \n",
       "availability_90                               3.204159e+01  0.000000e+00   \n",
       "availability_365                              1.303065e+02  0.000000e+00   \n",
       "number_of_reviews                             8.199650e+01  0.000000e+00   \n",
       "number_of_reviews_ltm                         2.122723e+01  0.000000e+00   \n",
       "number_of_reviews_l30d                        1.504259e+00  0.000000e+00   \n",
       "review_scores_rating                          4.660137e-01  0.000000e+00   \n",
       "review_scores_accuracy                        3.985007e-01  0.000000e+00   \n",
       "review_scores_cleanliness                     3.940967e-01  0.000000e+00   \n",
       "review_scores_checkin                         3.709812e-01  0.000000e+00   \n",
       "review_scores_communication                   3.849088e-01  1.000000e+00   \n",
       "review_scores_location                        3.518585e-01  1.000000e+00   \n",
       "review_scores_value                           4.603274e-01  1.000000e+00   \n",
       "calculated_host_listings_count                1.108793e+02  1.000000e+00   \n",
       "calculated_host_listings_count_entire_homes   9.364456e+01  0.000000e+00   \n",
       "calculated_host_listings_count_private_rooms  7.136186e+01  0.000000e+00   \n",
       "calculated_host_listings_count_shared_rooms   1.320940e-01  0.000000e+00   \n",
       "reviews_per_month                             1.910530e+00  1.000000e-02   \n",
       "\n",
       "                                                       25%           50%  \\\n",
       "id                                            2.515811e+07  4.824698e+07   \n",
       "scrape_id                                     2.023032e+13  2.023032e+13   \n",
       "host_id                                       2.234822e+07  1.074344e+08   \n",
       "host_listings_count                           2.000000e+00  1.200000e+01   \n",
       "host_total_listings_count                     3.000000e+00  1.600000e+01   \n",
       "neighbourhood_group_cleansed                           NaN           NaN   \n",
       "latitude                                      4.232098e+01  4.234448e+01   \n",
       "longitude                                    -7.110036e+01 -7.107307e+01   \n",
       "accommodates                                  2.000000e+00  2.000000e+00   \n",
       "bathrooms                                              NaN           NaN   \n",
       "bedrooms                                      1.000000e+00  1.000000e+00   \n",
       "beds                                          1.000000e+00  1.000000e+00   \n",
       "minimum_nights                                2.000000e+00  2.800000e+01   \n",
       "maximum_nights                                3.650000e+02  3.680000e+02   \n",
       "minimum_minimum_nights                        1.000000e+00  1.000000e+01   \n",
       "maximum_minimum_nights                        3.000000e+00  2.900000e+01   \n",
       "minimum_maximum_nights                        3.650000e+02  1.125000e+03   \n",
       "maximum_maximum_nights                        3.650000e+02  1.125000e+03   \n",
       "minimum_nights_avg_ntm                        2.200000e+00  2.800000e+01   \n",
       "maximum_nights_avg_ntm                        3.650000e+02  1.125000e+03   \n",
       "calendar_updated                                       NaN           NaN   \n",
       "availability_30                               0.000000e+00  7.000000e+00   \n",
       "availability_60                               0.000000e+00  2.200000e+01   \n",
       "availability_90                               6.000000e+00  4.000000e+01   \n",
       "availability_365                              7.700000e+01  2.210000e+02   \n",
       "number_of_reviews                             0.000000e+00  7.000000e+00   \n",
       "number_of_reviews_ltm                         0.000000e+00  1.000000e+00   \n",
       "number_of_reviews_l30d                        0.000000e+00  0.000000e+00   \n",
       "review_scores_rating                          4.600000e+00  4.800000e+00   \n",
       "review_scores_accuracy                        4.700000e+00  4.860000e+00   \n",
       "review_scores_cleanliness                     4.640000e+00  4.850000e+00   \n",
       "review_scores_checkin                         4.810000e+00  4.940000e+00   \n",
       "review_scores_communication                   4.790000e+00  4.920000e+00   \n",
       "review_scores_location                        4.670000e+00  4.875000e+00   \n",
       "review_scores_value                           4.500000e+00  4.700000e+00   \n",
       "calculated_host_listings_count                2.000000e+00  8.000000e+00   \n",
       "calculated_host_listings_count_entire_homes   1.000000e+00  4.000000e+00   \n",
       "calculated_host_listings_count_private_rooms  0.000000e+00  0.000000e+00   \n",
       "calculated_host_listings_count_shared_rooms   0.000000e+00  0.000000e+00   \n",
       "reviews_per_month                             2.700000e-01  1.000000e+00   \n",
       "\n",
       "                                                       75%           max  \n",
       "id                                            6.470705e+17  8.493358e+17  \n",
       "scrape_id                                     2.023032e+13  2.023032e+13  \n",
       "host_id                                       2.758496e+08  5.041795e+08  \n",
       "host_listings_count                           8.400000e+01  4.807000e+03  \n",
       "host_total_listings_count                     1.270000e+02  5.358000e+03  \n",
       "neighbourhood_group_cleansed                           NaN           NaN  \n",
       "latitude                                      4.235431e+01  4.239132e+01  \n",
       "longitude                                    -7.106081e+01 -7.099600e+01  \n",
       "accommodates                                  4.000000e+00  1.600000e+01  \n",
       "bathrooms                                              NaN           NaN  \n",
       "bedrooms                                      2.000000e+00  1.300000e+01  \n",
       "beds                                          2.000000e+00  2.200000e+01  \n",
       "minimum_nights                                3.200000e+01  4.000000e+02  \n",
       "maximum_nights                                1.125000e+03  1.125000e+03  \n",
       "minimum_minimum_nights                        3.200000e+01  4.000000e+02  \n",
       "maximum_minimum_nights                        9.100000e+01  4.000000e+02  \n",
       "minimum_maximum_nights                        1.125000e+03  2.147484e+09  \n",
       "maximum_maximum_nights                        1.125000e+03  2.147484e+09  \n",
       "minimum_nights_avg_ntm                        9.100000e+01  4.000000e+02  \n",
       "maximum_nights_avg_ntm                        1.125000e+03  2.147484e+09  \n",
       "calendar_updated                                       NaN           NaN  \n",
       "availability_30                               1.800000e+01  3.000000e+01  \n",
       "availability_60                               4.400000e+01  6.000000e+01  \n",
       "availability_90                               6.900000e+01  9.000000e+01  \n",
       "availability_365                              3.220000e+02  3.650000e+02  \n",
       "number_of_reviews                             4.400000e+01  8.210000e+02  \n",
       "number_of_reviews_ltm                         1.400000e+01  1.550000e+02  \n",
       "number_of_reviews_l30d                        0.000000e+00  2.200000e+01  \n",
       "review_scores_rating                          4.970000e+00  5.000000e+00  \n",
       "review_scores_accuracy                        4.990000e+00  5.000000e+00  \n",
       "review_scores_cleanliness                     4.990000e+00  5.000000e+00  \n",
       "review_scores_checkin                         5.000000e+00  5.000000e+00  \n",
       "review_scores_communication                   5.000000e+00  5.000000e+00  \n",
       "review_scores_location                        5.000000e+00  5.000000e+00  \n",
       "review_scores_value                           4.860000e+00  5.000000e+00  \n",
       "calculated_host_listings_count                5.000000e+01  3.400000e+02  \n",
       "calculated_host_listings_count_entire_homes   2.200000e+01  3.400000e+02  \n",
       "calculated_host_listings_count_private_rooms  4.000000e+00  2.740000e+02  \n",
       "calculated_host_listings_count_shared_rooms   0.000000e+00  4.000000e+00  \n",
       "reviews_per_month                             2.420000e+00  1.629000e+01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get basic stats on numerical data to check for negative numbers, nulls, and zeros.\n",
    "'''\n",
    "NOTE:  This was only for the purposes of developing the function below.\n",
    "'''\n",
    "\n",
    "lstn._get_numeric_data().describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e9f019-0e88-4e20-880e-a0dde33f005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The col_logger function will need to be brough in from the other notebooks\n",
    "\n",
    "This has been modified to include a 0 imputation value n such that transforamtion\n",
    "occurs on log(n) and not log(0) which is undefined.\n",
    "'''\n",
    "\n",
    "def col_logger(data_column, zero_imp = 1):\n",
    "    # Since log(0) is undefined, 0's must be treated as log(1)\n",
    "    return data_column.apply(lambda x: np.log(zero_imp) if x==0 else np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86081c09-6489-4a08-a952-305c59e5b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a logarithmic feature of an existing numerical feature and has built-in methods\n",
    "#  to handle zeros, negative numbers, and values between 0 and 1 which would produce negative numbers.\n",
    "\n",
    "def log_ft_maker(data_frame, training_data):\n",
    "    # Create a list of numerical columns\n",
    "    num_cols = list(data_frame._get_numeric_data().columns)\n",
    "\n",
    "    # Remove latitiude and longitude data as they were used previously to create distances\n",
    "    num_cols.remove('latitude')\n",
    "    num_cols.remove('longitude')\n",
    "\n",
    "    for col in num_cols:\n",
    "\n",
    "        # Find columsn with values between 0 and 1\n",
    "        if len(data_frame[col][(data_frame[col] < 1) & (data_frame[col] > 0)]) > 0:\n",
    "\n",
    "            # Determine the minimum value in that column, if it's 0, base the minimum\n",
    "            #  value off of the second smallest value in the column\n",
    "            # If it's negative, raise an error!\n",
    "            if min(data_frame[col]) < 0:\n",
    "                raise Exception('CANT LOGARITHM A NEGATIVE NUMBER')\n",
    "            \n",
    "            # Store the minimum column values to a global variable only if training_data == True\n",
    "            if training_data == True:\n",
    "                \n",
    "                # Handling zero imputation\n",
    "                if min(data_frame[col]) == 0:\n",
    "                    # second smallest value\n",
    "                    globals()[f'min_col_val{col}'] = data_frame[col].sort_values().unique()[1]\n",
    "                \n",
    "                # Handling 0 - 1 imputation\n",
    "                else:\n",
    "                    globals()[f'min_col_val{col}'] = min(data_frame[col][(data_frame[col] < 1) & (data_frame[col] > 0)])\n",
    "\n",
    "            # Calculate a zero imputation value for use in the col_logger function\n",
    "            #  Take the natural log of the minimum value and round down\n",
    "            '''\n",
    "            This last step ensures that any zero values will be less than any positive\n",
    "            values after a log transformation.\n",
    "            '''\n",
    "            z_imp = np.exp(np.floor(np.log(globals()[f'min_col_val{col}'])))\n",
    "\n",
    "            # Transform the column:\n",
    "            data_frame[f'log_{col}'] = col_logger(data_frame[col], z_imp)\n",
    "\n",
    "        else:\n",
    "            # Otherwise, simply use defulat zero_imputation value of 1\n",
    "            data_frame[f'log_{col}'] = col_logger(data_frame[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753a598-26fb-443f-a5c9-9ff65090ffbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8 - One Hot Encoding\n",
    "Most categorical features save for those described in Notebook 2 are one-hot-encoded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3287d6d4-6a71-4fa6-96bf-f468f65addc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help with this text from the documentation:  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "def ohe_fn_2(data_frame, training_data):\n",
    "    # Find the remaining categorical columns:\n",
    "    cat_cols = list(data_frame.columns)\n",
    "    num_cols = list(data_frame._get_numeric_data().columns)\n",
    "\n",
    "    for col in num_cols:\n",
    "        cat_cols.remove(col)\n",
    "\n",
    "    # Some of these columns must be removed to be handled separately in advanced processing\n",
    "    remove_cols = ['amenities', 'host_about', 'name', 'neighborhood_overview', 'description']\n",
    "\n",
    "    for col in remove_cols:\n",
    "        cat_cols.remove(col)\n",
    "\n",
    "    # Create a new temporary dataframe with just the columns taht nee to be one hot encoded\n",
    "    #  otherwise, it will try to OHE the whole thing...\n",
    "    data_frame_temp = data_frame[cat_cols]\n",
    "    \n",
    "    # Add an if statment to fit_transform only if it's the training data:\n",
    "    if training_data == True:\n",
    "        # The instance of onehotencoder must be created as aglabal variable\n",
    "        #  ONLY DO THIS FOR TRAINING!!!\n",
    "        globals()['ohe_inst'] = OneHotEncoder(sparse_output = False, drop = 'first', handle_unknown='ignore')\n",
    "        \n",
    "        # Create the ohe array from the temporary dataframe (fitted and transformed)\n",
    "        ohe_array = ohe_inst.fit_transform(data_frame_temp)\n",
    "\n",
    "    else:\n",
    "        # Create the ohe array from the temporary dataframe (transformed only)\n",
    "        ohe_array = ohe_inst.transform(data_frame_temp)\n",
    "    \n",
    "    # Make a dataframe from the array with the proper columns\n",
    "    ohe_array_df = pd.DataFrame(ohe_array, columns=ohe_inst.get_feature_names_out())\n",
    "    \n",
    "    # Drop the original cat_col columns in the original dataframe\n",
    "    data_frame.drop(columns=cat_cols, inplace=True)\n",
    "    \n",
    "    # Merge the OHE data into the new dataframe\n",
    "    data_frame = pd.merge(left = data_frame, \n",
    "                 right = ohe_array_df, \n",
    "                 left_index=True, right_index=True, \n",
    "                 how = 'outer')\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae31b4e-7ebb-463d-a9b5-f19118bb5c4c",
   "metadata": {},
   "source": [
    "# 9 - Advanced Processing\n",
    "Here, the features created in Notebook 2 are replicated here in function form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b9b15-600f-4167-beea-a55e0d705a30",
   "metadata": {},
   "source": [
    "## 9.1 - Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c4bb45-0145-4a16-ac27-ab8ccc600451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to merging, add a prefix to the column names since it is text data and words tokenized from\n",
    "#  one column could overwrite with those of another.\n",
    "\n",
    "# Make a function to do this for all dataframes.\n",
    "\n",
    "def col_renamer(df, prefix):\n",
    "    new_names = [f'{prefix}_{col}' for col in df.columns]\n",
    "    df.columns = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fe73ea-5551-4358-a4c8-1a046a4f2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize the regex and code from the previous notebook.\n",
    "\n",
    "def amentiy_count_maker(data_frame):\n",
    "    # Use a regular expression to extract the amenities which are between quotes.\n",
    "    #  Code adapted from this source: https://stackoverflow.com/questions/1454913/regular-expression-to-find-a-string-included-between-two-characters-while-exclud\n",
    "    # Also helpful:  https://regex101.com/\n",
    "    regex_string = '(?<=\")[^\"]+(?=\"[,\\]])'\n",
    "    \n",
    "    # Using regex as before, find the number of amenities for every listing and store to a new column\n",
    "    data_frame['amen_cnt'] = data_frame.amenities.apply(lambda x: len(re.findall(regex_string, x)))\n",
    "\n",
    "    # Create a log transformed column, setting the zero imputation value to e^-1\n",
    "    data_frame['log_amen_cnt'] = col_logger(data_frame['amen_cnt'], np.exp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d4db92-bec8-42a7-b264-6e2ed19ac8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize the regex and code from the previous notebook.\n",
    "\n",
    "def amenity_maker(data_frame, training_data):\n",
    "    # Use a regular expression to extract the amenities which are between quotes.\n",
    "    #  Code adapted from this source: https://stackoverflow.com/questions/1454913/regular-expression-to-find-a-string-included-between-two-characters-while-exclud\n",
    "    # Also helpful:  https://regex101.com/\n",
    "    regex_string = '(?<=\")[^\"]+(?=\"[,\\]])'\n",
    "\n",
    "    amn_lst = []\n",
    "\n",
    "    for string_lists in data_frame.amenities:\n",
    "        a_list = re.findall(regex_string, string_lists)\n",
    "        for amenity in a_list:\n",
    "            amn_lst.append(amenity)\n",
    "\n",
    "    # Create a pandas series of all amenities and their number of occurences\n",
    "    amn_counts = pd.Series(amn_lst).value_counts(ascending=False)\n",
    "\n",
    "    # Filter the datafarme to use only words that appear in 99% of posts\n",
    "    #  THIS IS REQUIRED GIVEN THAT min_df IS IGNORED BY COUNT VECTORISZER WITH CUSTOM DICTIONARIES\n",
    "    #  Create a vocab variable by using the index attribute to get the list of amenities\n",
    "    amn_vocab = amn_counts[amn_counts >= 35].index\n",
    "\n",
    "    if training_data == True:\n",
    "        # Use countevectorizer to one hot encode all the amenities\n",
    "        #  Use the vocab to get only the amenities encoded\n",
    "        #  NOTE:  Set the 'token_pattern' to the regex string so it finds the exact same tokens as were found previously\n",
    "        globals()[f'cvec_amen'] = CountVectorizer(lowercase=False,\n",
    "                               vocabulary=amn_vocab,\n",
    "                               ngram_range=(1, 1),\n",
    "                               token_pattern=regex_string,\n",
    "                              )\n",
    "\n",
    "        # Create a new dataframe with the count vectorized data from the amenities column\n",
    "        amen_df = pd.DataFrame(globals()[f'cvec_amen'].fit_transform(data_frame.amenities).todense(), \n",
    "                     columns = globals()[f'cvec_amen'].get_feature_names_out())\n",
    "    \n",
    "    else:\n",
    "        # Create a new dataframe with the count vectorized data from the amenities column\n",
    "        amen_df = pd.DataFrame(globals()[f'cvec_amen'].transform(data_frame.amenities).todense(), \n",
    "                     columns = globals()[f'cvec_amen'].get_feature_names_out())\n",
    "        \n",
    "    # Rename the columns of amen_df\n",
    "    col_renamer(amen_df, 'amen')\n",
    "    \n",
    "    # Drop the amenities column from the original dataframe\n",
    "    data_frame.drop(columns = 'amenities', inplace = True)\n",
    "    \n",
    "    # Merge amen_df into the lstn dataframe\n",
    "    return pd.merge(left = data_frame, \n",
    "                    right = amen_df, \n",
    "                    left_index=True, right_index=True, \n",
    "                    how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06e100-9cfb-49d3-a28e-16ff9129ee9a",
   "metadata": {},
   "source": [
    "## 9.2 - Count Vectorize and Add Columns for Name, Description, Neigb. Overview, and Host About"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7ab2670-1668-4b9e-8f68-23eb180aff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function that will:\n",
    "* vectorize the columns\n",
    "* create a new df for those columns\n",
    "* rename those columns\n",
    "* merge into lstn\n",
    "* Store the count vectorizer objects so that .transform can be later \n",
    "run on testing/validation datasets\n",
    "'''\n",
    "\n",
    "def text_col_cvec(data_frame, training_data):\n",
    "    \n",
    "    # Provide all the columns that need to be word vectorized\n",
    "    text_cols = ['name', 'description', 'host_about', 'neighborhood_overview']\n",
    "    \n",
    "    for col in text_cols:\n",
    "        # print(f'before: {data_frame.shape}')\n",
    "        \n",
    "        # Create a temporary variable to establish the pandas series based on the list\n",
    "        column_data = data_frame[col]\n",
    "        \n",
    "        # Do fit_transform only if training_data = True\n",
    "        if training_data == True:\n",
    "            \n",
    "            # Instantiate count vectorizer\n",
    "            globals()[f'cvec_{col}'] = CountVectorizer(ngram_range=(1, 4), min_df=0.01)\n",
    "\n",
    "            # Create a new dataframe with the count vectorized data from the selected column\n",
    "            cvec_df = pd.DataFrame(globals()[f'cvec_{col}'].fit_transform(column_data).todense(), \n",
    "                         columns = globals()[f'cvec_{col}'].get_feature_names_out())\n",
    "            \n",
    "        else:\n",
    "            # Create a new dataframe with the count vectorized data from the selected column\n",
    "            cvec_df = pd.DataFrame(globals()[f'cvec_{col}'].transform(column_data).todense(), \n",
    "                         columns = globals()[f'cvec_{col}'].get_feature_names_out())\n",
    "\n",
    "        # Rename the columns\n",
    "        col_renamer(cvec_df, col)\n",
    "        \n",
    "        # print(cvec_df.shape)\n",
    "        \n",
    "        \n",
    "        # Drop the newly vecotirzed column from the original dataframe\n",
    "        data_frame.drop(columns = col, inplace = True)\n",
    "\n",
    "        # Merge into the main dataframe\n",
    "        data_frame = pd.merge(left = data_frame, \n",
    "             right = cvec_df, \n",
    "             left_index=True, right_index=True, \n",
    "             how = 'outer')\n",
    "        \n",
    "        # print(f'after: {data_frame.shape} \\n')\n",
    "        \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff64ca-ca4f-434d-b3de-02a868dbab4e",
   "metadata": {},
   "source": [
    "# 10 - Combine All Functions Into One and Tranform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea67319-2f7f-43ca-8ec5-61937446f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_formatter_2(data_frame, training_data):\n",
    "    # wrap in a try/except block for easier debugging\n",
    "    try:\n",
    "        print(f'start: {data_frame.shape[1]}')\n",
    "\n",
    "        col_dropper(data_frame)\n",
    "        print(f'col_dropper: {data_frame.shape[1]}')\n",
    "\n",
    "        data_fixer(data_frame)\n",
    "        print(f'data_fixer: {data_frame.shape[1]}')\n",
    "\n",
    "        data_imputer(data_frame, training_data)\n",
    "        print(f'data_imputer: {data_frame.shape[1]}')\n",
    "\n",
    "        simp_num_ft(data_frame)\n",
    "        print(f'simp_num_ft: {data_frame.shape[1]}')\n",
    "\n",
    "        dist_ft_adder(data_frame)\n",
    "        print(f'dist_ft_adder: {data_frame.shape[1]}')\n",
    "\n",
    "        log_ft_maker(data_frame, training_data)\n",
    "        print(f'log_ft_maker: {data_frame.shape[1]}')\n",
    "\n",
    "        data_frame = ohe_fn_2(data_frame, training_data)\n",
    "        print(f'ohe_fn: {data_frame.shape[1]}')\n",
    "\n",
    "        amentiy_count_maker(data_frame)\n",
    "        print(f'amentiy_count_maker: {data_frame.shape[1]}')\n",
    "\n",
    "        data_frame = amenity_maker(data_frame, training_data)\n",
    "        print(f'amenity_maker: {data_frame.shape[1]}')\n",
    "\n",
    "        data_frame = text_col_cvec(data_frame, training_data)\n",
    "        print(f'text_col_cvec: {data_frame.shape[1]}')\n",
    "\n",
    "        return data_frame\n",
    "\n",
    "    # This will be more explicit about which line the error occirs at\n",
    "    except Exception:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77f6d4b6-7163-4427-baf4-507d4db93045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 75\n",
      "col_dropper: 41\n",
      "data_fixer: 41\n",
      "data_imputer: 41\n",
      "simp_num_ft: 44\n",
      "dist_ft_adder: 45\n",
      "log_ft_maker: 72\n",
      "ohe_fn: 156\n",
      "amentiy_count_maker: 158\n",
      "amenity_maker: 300\n",
      "text_col_cvec: 12582\n"
     ]
    }
   ],
   "source": [
    "# Import the Training Data\n",
    "lstn = master_formatter_2(lstn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a9a0292-c226-4102-8110-f2ea7f1e8a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 75\n",
      "col_dropper: 41\n",
      "data_fixer: 41\n",
      "data_imputer: 41\n",
      "simp_num_ft: 44\n",
      "dist_ft_adder: 45\n",
      "log_ft_maker: 72\n",
      "ohe_fn: 156\n",
      "amentiy_count_maker: 158\n",
      "amenity_maker: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:202: UserWarning: Found unknown categories in columns [6, 8] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_col_cvec: 12582\n",
      "start: 75\n",
      "col_dropper: 41\n",
      "data_fixer: 41\n",
      "data_imputer: 41\n",
      "simp_num_ft: 44\n",
      "dist_ft_adder: 45\n",
      "log_ft_maker: 72\n",
      "ohe_fn: 156\n",
      "amentiy_count_maker: 158\n",
      "amenity_maker: 300\n",
      "text_col_cvec: 12582\n"
     ]
    }
   ],
   "source": [
    "# Read in the testing and validation data to be transformed\n",
    "validation = pd.read_csv('../data/listings_val.csv')\n",
    "testing = pd.read_csv('../data/listings_test.csv')\n",
    "\n",
    "# Run the formatting function on the data\n",
    "validation = master_formatter_2(validation, False)\n",
    "testing = master_formatter_2(testing, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59af5783-4171-4711-b380-1524140f94cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((696, 12582), (387, 12582))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shapes to verify proper column count\n",
    "validation.shape, testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f297623-a123-48fe-9578-d722fe501b79",
   "metadata": {},
   "source": [
    "# 11 - Export Data to .csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "278bf259-eb11-40b4-8b4e-fb5860fa30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstn.to_csv('../data/prepped_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffcdde5b-78a1-4b5f-8283-50695bc06637",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.to_csv('../data/prepped_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ed1241-6d8c-4442-b65e-5df1e37216a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.to_csv('../data/prepped_testing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
